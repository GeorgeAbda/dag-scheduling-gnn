\subsection{Problem Formulation}

\label{sec:PF}

We adapt the MDP formulation from \cite{sensors-paper} for workflow scheduling on virtualized clusters, extending it to support \emph{concurrent task execution} on multi-core VMs with CPU fractionation and memory-aware resource allocation.
We formulate the problem as a finite-horizon Markov Decision Process (MDP)
$\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$ over decision epochs
$k=0,1,\dots,K$, aligned with scheduling decisions.

\paragraph{System Model.}
Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ be a workflow DAG with tasks $i\in\mathcal{V}$ and precedence edges
$(p\!\to\!i)\in\mathcal{E}$. Each task has:
(i) computational size $L_i$ (e.g., MI),
(ii) resource demand vector $\mathbf{d}_i=(\mathrm{cpu}_i,\mathrm{mem}_i)$ specifying required CPU cores and memory,
(iii) compatibility set $\mathcal{C}_i\subseteq\mathcal{M}$ of admissible VMs.
Each VM $m\in\mathcal{M}$ has capacity $\mathbf{c}_m=(C^{\mathrm{cpu}}_m,C^{\mathrm{mem}}_m)$,
processing speed $s_m$ (e.g., MIPS), and power parameters $(P^{\mathrm{idle}}_m,P^{\mathrm{peak}}_m)$.

\paragraph{Decision Epochs and Clock.}
Let $\tau_k$ denote the simulation clock at decision epoch $k$. Decisions occur when the agent assigns a ready task.
Between decisions, the environment advances $\tau$ according to scheduled start/finish events implied by previous assignments.

\paragraph{State Space $\mathcal{S}$.}
A state $s_k\in\mathcal{S}$ summarizes all information needed for optimal control:
\begin{align*}
s_k = (&\tau_k; 
\{\text{task status}_i \in \{\text{not\_ready},\text{ready},\text{running},\text{done}\}\}_{i\in\mathcal{V}};\\
&\{\text{parent\_ready}_i=\max_{p\in\mathrm{Pa}(i)} c_p\}_{i\in\mathcal{V}};
\{\text{assignment}_i\in \mathcal{C}_i\cup\{\varnothing\}\}_{i\in\mathcal{V}};
\{\text{start}_i,\,c_i\}_{i\in\mathcal{V}};\\
&\{\text{VM residual capacities and active allocations over }[\tau_k,\infty)\}_{m\in\mathcal{M}};
\{\mathcal{C}_i\}_{i\in\mathcal{V}} ).
\end{align*}
The ready set at $\tau_k$ is $\mathcal{R}_k=\{i:\ \text{task status}_i=\text{ready}\}$.
The state is fully observable; thus the Markov property holds.

\paragraph{Action Space $\mathcal{A}$.}
An action selects a task--VM pair:
\[
a_k=(i,m)\in \mathcal{F}(s_k)\subseteq \mathcal{V}\times\mathcal{M},
\]
where the feasible set enforces precedence, compatibility, and capacity:
\[
\mathcal{F}(s_k)=\Bigl\{(i,m):\ i\in\mathcal{R}_k,\ m\in\mathcal{C}_i,\ 
\mathbf{d}_i \preceq \text{residual\_capacity}_m(t)\ \text{for some } t\ge \text{parent\_ready}_i \Bigr\}.
\]
\textbf{Extension for concurrent execution:} Unlike \cite{sensors-paper}, which assumes one task per VM at a time, we allow multiple tasks to execute concurrently on a VM as long as aggregate resource demands respect VM capacities. 
Operationally, assigning $(i,m)$ schedules task $i$ on VM $m$ at its
earliest feasible start time
\[
s_i = \min\{t\ge \text{parent\_ready}_i:\ \mathbf{d}_i \preceq \text{residual\_capacity}_m(t)\},
\qquad
c_i = s_i + \frac{L_i}{s_m},
\]
where $\text{residual\_capacity}_m(t)=(C^{\mathrm{cpu}}_m - \sum_{j\in A_m(t)}\mathrm{cpu}_j,\, C^{\mathrm{mem}}_m - \sum_{j\in A_m(t)}\mathrm{mem}_j)$ accounts for all tasks $A_m(t)$ active on VM $m$ at time $t$.
The assignment updates VM $m$'s capacity timeline accordingly. Action masking enforces $\mathcal{F}(s_k)$.

\paragraph{Transition Kernel $\mathcal{P}$.}
Given $s_k$ and $a_k=(i,m)$, the environment deterministically updates:
(i) task $i$'s $(\text{start}_i,c_i,\text{status}_i)$,
(ii) VM $m$'s allocation timeline with concurrent resource tracking,
(iii) descendants' readiness when all parents are completed:
$\text{task status}_j\gets\text{ready}$ if $\forall p\in \mathrm{Pa}(j): \text{task status}_p=\text{done}$ and $\tau\ge \max_{p\in\mathrm{Pa}(j)} c_p$,
(iv) simulation clock to the next decision epoch $\tau_{k+1}$.


\paragraph{Reward $\mathcal{R}$ as Regret Shaping.}
\label{par:reward}
We extend the reward formulation of \cite{sensors-paper} by defining per-step rewards as \emph{regret reductions} relative to integrated heuristic estimates that account for concurrent execution.
Let
\[
\widehat{T}(s)\equiv \texttt{obs.makespan()},\qquad
\widehat{E}(s)\equiv \texttt{obs.energy\_consumption()}
\]
where $\widehat{T}(s)$ is a concurrency-aware greedy earliest-completion-time estimate that simulates per-VM event timelines respecting memory and core capacities for overlapping tasks, and $\widehat{E}(s)$ integrates VM power
\[
P_m(t)=P^{\mathrm{idle}}_m+\bigl(P^{\mathrm{peak}}_m-P^{\mathrm{idle}}_m\bigr)\,U_m(t)
\]
piecewise-constant over the same timelines, with CPU utilization
\[
U_m(t)=\min\!\left(1,\ \frac{1}{C^{\mathrm{cpu}}_m}\sum_{i\in A_m(t)} \mathrm{cpu}_i\right)
\]
reflecting fractional core usage by all active tasks $A_m(t)$ on VM $m$.

We define per-step regret reductions between $s_{k}$ and $s_{k+1}$ as
\[
\Delta R^{\mathrm{mk}}_k \equiv -\,\frac{\widehat{T}(s_{k+1})-\widehat{T}(s_k)}{\max\!\bigl(\widehat{T}(s_{k+1}),\varepsilon\bigr)},
\qquad
\Delta R^{\mathrm{en}}_k \equiv -\,\frac{\widehat{E}(s_{k+1})-\widehat{E}(s_k)}{\max\!\bigl(\widehat{E}(s_{k+1}),\varepsilon\bigr)},
\]
so a positive value indicates the action decreased the heuristic cost-to-go. The shaped multi-objective reward is
\[
r_k = w_T\,\Delta R^{\mathrm{mk}}_k + w_E\,\Delta R^{\mathrm{en}}_k,
\]
which preserves credit assignment across scheduling decisions while scalarizing makespan and active energy objectives.

\textbf{Key differences from \cite{sensors-paper}:}
\begin{itemize}
\item \textbf{Concurrent execution model:} Our makespan estimate $\widehat{T}(s)$ and energy estimate $\widehat{E}(s)$ account for multiple overlapping tasks per VM, whereas the original formulation assumes sequential task execution.
\item \textbf{CPU fractionation:} Energy calculation uses fractional CPU utilization $U_m(t)$ based on aggregate core demands of concurrent tasks, enabling more accurate power modeling for multi-core VMs.
\item \textbf{Dynamic capacity tracking:} VM resource availability evolves continuously as tasks start and complete, requiring event-based timeline simulation for both makespan and energy heuristics.
\end{itemize}

\paragraph{Objective.}
We optimize a stationary policy $\pi_\theta(a\mid s)$ to maximize expected return
\[
J(\pi_\theta) = \mathbb{E}_{\pi_\theta,\mathcal{P}}\!\left[\sum_{k=0}^{K} \gamma^k\, r_k\right],
\]
typically with $\gamma\approx 1$ for episodic scheduling. The weights $(w_T,w_E)$
scalarize the multi-objective (makespan, energy) into a single reward while preserving credit assignment via per-decision regret deltas.
A terminal evaluation computes final $T_{\mathrm{mk}}=\max_i c_i$ and total energy
\[
E_{\mathrm{tot}} = \sum_{m\in\mathcal{M}} \int_{0}^{T_{\mathrm{mk}}} P_m(t)\,dt
\]
once all tasks are completed.

\paragraph{Constraints and Termination.}
Feasibility is enforced by $\mathcal{F}(s_k)$:
(i) precedence constraints via readiness,
(ii) per-VM resource capacities (CPU cores and memory) over time with concurrent task support,
(iii) task--VM compatibility.
The episode terminates when all tasks are completed; then $T_{\mathrm{mk}}$ and $E_{\mathrm{tot}}$ are computed.
