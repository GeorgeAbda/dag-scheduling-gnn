%==============================================================================
% OPTIMAL TRANSPORT EXTENSION
% Add this section to the main paper
%==============================================================================

\section{Optimal Transport for Gradient Distribution Matching}
\label{sec:ot}

While the basic GDD method compares mean gradient vectors, we now extend it using Optimal Transport (OT) to compare the \textit{distributions} of gradients. This captures richer information about domain structure and achieves perfect clustering in our experiments.

\subsection{From Point Estimates to Distributions}

Instead of computing a single gradient signature per domain, we collect multiple gradient samples:
\begin{equation}
    \mu_i = \frac{1}{N} \sum_{n=1}^{N} \delta_{g_i^{(n)}}
\end{equation}
where $g_i^{(n)}$ is the $n$-th gradient sample from domain $\mathcal{M}_i$, and $\delta_x$ denotes a Dirac measure at $x$. This empirical distribution $\mu_i$ captures the variability in gradients due to:
\begin{itemize}
    \item Stochastic policy sampling
    \item Different preference weights $\alpha$
    \item Random policy initializations
\end{itemize}

\subsection{Wasserstein Distance for Domain Similarity}

We measure domain similarity using the 2-Wasserstein distance:
\begin{equation}
    W_2(\mu_i, \mu_j) = \left( \inf_{\gamma \in \Pi(\mu_i, \mu_j)} \int \|g - g'\|^2 \, d\gamma(g, g') \right)^{1/2}
\end{equation}
where $\Pi(\mu_i, \mu_j)$ is the set of all couplings (joint distributions) with marginals $\mu_i$ and $\mu_j$.

\begin{theorem}[OT Domain Separation]
\label{thm:ot_separation}
Let $\mu_i$ and $\mu_j$ be gradient distributions from domains $\mathcal{M}_i$ and $\mathcal{M}_j$. If the domains have:
\begin{enumerate}
    \item Identical reward structures: $W_2(\mu_i, \mu_j) \rightarrow 0$ as $N \rightarrow \infty$
    \item Different reward structures: $W_2(\mu_i, \mu_j) \geq c > 0$ for some constant $c$
\end{enumerate}
\end{theorem}

\begin{proof}[Proof Sketch]
For identical domains, the gradient distributions converge to the same population distribution, so $W_2 \rightarrow 0$. For different domains, the support of the distributions differs (gradients point in different directions), ensuring a positive lower bound on $W_2$.
\end{proof}

\subsection{Sliced Wasserstein for Scalability}

Computing exact $W_2$ requires solving a linear program with $O(N^3)$ complexity. For high-dimensional gradients, we use the Sliced Wasserstein distance:
\begin{equation}
    SW_2(\mu_i, \mu_j) = \left( \int_{\mathbb{S}^{d-1}} W_2^2(\text{proj}_\theta \mu_i, \text{proj}_\theta \mu_j) \, d\sigma(\theta) \right)^{1/2}
\end{equation}
where $\text{proj}_\theta$ projects onto the direction $\theta$, and $\sigma$ is the uniform measure on the unit sphere.

\begin{proposition}[Sliced Wasserstein Properties]
The Sliced Wasserstein distance satisfies:
\begin{enumerate}
    \item $SW_2$ is a valid metric on probability distributions
    \item $SW_2(\mu, \nu) \leq W_2(\mu, \nu)$ (upper bounded by exact Wasserstein)
    \item Computable in $O(N \log N)$ per projection (vs. $O(N^3)$ for exact)
    \item Preserves separation: if $W_2(\mu_i, \mu_j) \geq c$, then $SW_2(\mu_i, \mu_j) \geq c' > 0$
\end{enumerate}
\end{proposition}

\subsection{OT Barycenters for Domain Prototypes}

Given a cluster of domains $\{\mathcal{M}_{i_1}, \ldots, \mathcal{M}_{i_k}\}$, we compute a prototype distribution via the Wasserstein barycenter:
\begin{equation}
    \bar{\mu} = \arg\min_{\mu} \sum_{j=1}^{k} w_j W_2^2(\mu, \mu_{i_j})
\end{equation}
where $w_j$ are weights (uniform by default). This barycenter represents the ``canonical'' gradient distribution for the cluster.

\begin{proposition}[Barycenter Interpretation]
The Wasserstein barycenter $\bar{\mu}$ minimizes the total transport cost from all cluster members. It can be interpreted as:
\begin{itemize}
    \item The ``average'' domain in Wasserstein space
    \item A prototype for transfer learning to new domains
    \item A reference point for measuring domain shift
\end{itemize}
\end{proposition}

\subsection{Gradient Flow Perspective}

Policy optimization can be viewed as gradient flow in Wasserstein space. Let $\rho_t$ denote the distribution of policy parameters at time $t$. Under gradient descent on the scalarized objective:
\begin{equation}
    \frac{\partial \rho_t}{\partial t} = \nabla \cdot (\rho_t \nabla_\theta J_\alpha)
\end{equation}
This is the continuity equation with velocity field $v = -\nabla_\theta J_\alpha$.

\begin{theorem}[Domain-Specific Velocity Fields]
Different domains induce different velocity fields in parameter space:
\begin{equation}
    v_i(\theta) = -\nabla_\theta J_\alpha(\theta; \mathcal{M}_i)
\end{equation}
The Wasserstein distance between gradient distributions measures the ``difference in optimization dynamics'' between domains.
\end{theorem}

This perspective connects domain discovery to the geometry of optimization: domains with similar velocity fields (similar gradients) will have similar optimization trajectories.

\subsection{Algorithm: OT-Enhanced Domain Discovery}

\begin{algorithm}[t]
\caption{OT-Enhanced Domain Discovery}
\label{alg:ot_gdd}
\begin{algorithmic}[1]
\REQUIRE Domains $\{\mathcal{M}_1, \ldots, \mathcal{M}_n\}$, clusters $K$, samples $N$
\ENSURE Cluster assignments, domain prototypes

\FOR{$i = 1$ to $n$}
    \STATE Collect $N$ gradient samples: $\{g_i^{(1)}, \ldots, g_i^{(N)}\}$
    \STATE Form empirical distribution $\mu_i$
\ENDFOR

\FOR{$i = 1$ to $n$}
    \FOR{$j = i+1$ to $n$}
        \STATE Compute $D_{ij} = SW_2(\mu_i, \mu_j)$ \COMMENT{Sliced Wasserstein}
    \ENDFOR
\ENDFOR

\STATE Convert to similarity: $S = 1 - D / \max(D)$
\STATE Apply spectral clustering to $S$ with $K$ clusters
\STATE Compute Wasserstein barycenter for each cluster

\RETURN Cluster assignments, barycenters
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Results}

\begin{table}[t]
\centering
\caption{Comparison of similarity metrics for domain discovery}
\label{tab:ot_results}
\begin{tabular}{lccc}
\toprule
Method & ARI $\uparrow$ & NMI $\uparrow$ & Complexity \\
\midrule
Cosine Similarity (GDD) & 0.64 & 0.79 & $O(d)$ \\
Sinkhorn Distance & 0.35 & 0.65 & $O(N^2)$ \\
\textbf{Sliced Wasserstein} & \textbf{1.00} & \textbf{1.00} & $O(N \log N)$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:ot_results} shows that Sliced Wasserstein achieves \textbf{perfect clustering} (ARI = 1.0) on MO-Gymnasium benchmarks. Key observations:

\begin{enumerate}
    \item \textbf{Within-domain SW distance = 0}: Gradient distributions from the same environment type are identical (up to sampling noise).
    
    \item \textbf{Cross-domain SW distance > 0}: Different environment types have clearly separated gradient distributions.
    
    \item \textbf{Sinkhorn underperforms}: Entropy regularization blurs the differences between distributions, reducing separation.
\end{enumerate}

\subsection{Why OT Outperforms Cosine Similarity}

\begin{proposition}[Information Captured by OT]
The Wasserstein distance captures:
\begin{enumerate}
    \item \textbf{Location}: Where gradients point (like cosine similarity)
    \item \textbf{Spread}: Variance in gradient directions
    \item \textbf{Shape}: Higher-order structure of the distribution
\end{enumerate}
Cosine similarity only captures (1), while OT captures all three.
\end{proposition}

This explains the performance gap: OT leverages the full distributional structure of gradients, not just their mean direction.

%==============================================================================
% CONNECTION TO DADIL (Dataset Dictionary Learning)
%==============================================================================

\subsection{Connection to Dataset Dictionary Learning}

Our OT-based approach connects naturally to Dataset Dictionary Learning (DaDiL) \citep{dadil2023}. In DaDiL, datasets are represented as distributions and compared using Wasserstein distance. We extend this to the gradient space:

\begin{definition}[Gradient Dictionary]
A gradient dictionary is a set of prototype distributions $\{\bar{\mu}_1, \ldots, \bar{\mu}_K\}$ (Wasserstein barycenters) that represent canonical domain types.
\end{definition}

Given a new domain $\mathcal{M}_{\text{new}}$, we can:
\begin{enumerate}
    \item Compute its gradient distribution $\mu_{\text{new}}$
    \item Find the nearest prototype: $k^* = \arg\min_k W_2(\mu_{\text{new}}, \bar{\mu}_k)$
    \item Transfer knowledge from domains in cluster $k^*$
\end{enumerate}

This provides a principled framework for domain-aware transfer learning in MORL.

\subsection{Theoretical Guarantees}

\begin{theorem}[Clustering Consistency]
\label{thm:consistency}
Under mild regularity conditions, as the number of gradient samples $N \rightarrow \infty$:
\begin{enumerate}
    \item The empirical Sliced Wasserstein distance converges: $SW_2(\hat{\mu}_i, \hat{\mu}_j) \rightarrow SW_2(\mu_i, \mu_j)$
    \item Spectral clustering on the SW distance matrix is consistent: the recovered clusters converge to the true domain partition
\end{enumerate}
\end{theorem}

\begin{proof}[Proof Sketch]
Convergence of $SW_2$ follows from the law of large numbers for empirical measures. Clustering consistency follows from the stability of spectral clustering under perturbations of the affinity matrix \citep{von2007tutorial}.
\end{proof}
