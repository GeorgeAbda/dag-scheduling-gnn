%==============================================================================
% GRADIENT CONFLICT WITHIN ENVIRONMENTS
% The key insight: same environment, same α, different trajectories → conflicting gradients
%==============================================================================

\section{Intra-Domain Gradient Conflict}
\label{sec:gradient_conflict}

A fundamental challenge in multi-objective reinforcement learning is that \textbf{different trajectories within the same environment can produce conflicting policy gradients}, even when using the same preference weight $\alpha$. This section formalizes this phenomenon and explains its implications for domain discovery.

\subsection{The Trajectory-Dependent Gradient Problem}

Consider a multi-objective MDP $\mathcal{M}$ with two objectives. For a fixed preference weight $\alpha$ and policy $\pi_\theta$, the policy gradient is:
\begin{equation}
    \nabla_\theta J_\alpha(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t^\alpha \right]
\end{equation}
where $G_t^\alpha = \sum_{t'=t}^{T} \gamma^{t'-t} [\alpha r_1(s_{t'}, a_{t'}) + (1-\alpha) r_2(s_{t'}, a_{t'})]$.

\paragraph{Key Observation.} The gradient depends on the \textit{specific trajectory} $\tau$ sampled. Different trajectories can yield gradients pointing in \textbf{opposite directions}:

\begin{definition}[Trajectory Gradient Conflict]
Two trajectories $\tau_1$ and $\tau_2$ from the same environment exhibit gradient conflict if:
\begin{equation}
    \cos\left( g(\tau_1), g(\tau_2) \right) < 0
\end{equation}
where $g(\tau) = \sum_{t} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t^\alpha$ is the per-trajectory gradient estimate.
\end{definition}

\subsection{Sources of Intra-Domain Gradient Conflict}

Gradient conflict within a single environment arises from several sources:

\subsubsection{1. Objective Trade-off Along Trajectories}

In multi-objective environments, a single trajectory may achieve high reward on one objective while sacrificing the other. Consider Deep Sea Treasure:

\begin{itemize}
    \item \textbf{Trajectory A}: Agent reaches nearby treasure quickly
        \begin{itemize}
            \item High reward on Objective 2 (time efficiency)
            \item Low reward on Objective 1 (treasure value)
            \item Gradient encourages: ``repeat this fast behavior''
        \end{itemize}
    \item \textbf{Trajectory B}: Agent reaches distant treasure slowly
        \begin{itemize}
            \item Low reward on Objective 2 (time efficiency)
            \item High reward on Objective 1 (treasure value)
            \item Gradient encourages: ``repeat this value-seeking behavior''
        \end{itemize}
\end{itemize}

Even with the same $\alpha = 0.5$, these trajectories produce gradients pointing in different directions because they represent different Pareto-optimal strategies.

\subsubsection{2. Stochastic Policy Exploration}

A stochastic policy $\pi_\theta$ samples different actions, leading to different state-action sequences:
\begin{equation}
    a_t \sim \pi_\theta(\cdot | s_t)
\end{equation}

Two rollouts from the same initial state can diverge significantly, visiting different regions of the state space and receiving different reward signals.

\subsubsection{3. Reward Sparsity and Variance}

In environments with sparse rewards (like Deep Sea Treasure), most trajectories receive zero reward until reaching a goal. This creates high variance:
\begin{itemize}
    \item Trajectories that reach a goal: large gradient magnitude
    \item Trajectories that fail: near-zero gradient
\end{itemize}

The direction of the gradient depends entirely on \textit{which} goal was reached.

\subsection{Formalizing Gradient Conflict}

Let $\mathcal{T}$ be the distribution of trajectories induced by policy $\pi_\theta$ in environment $\mathcal{M}$. We define:

\begin{definition}[Intra-Domain Conflict Rate]
The conflict rate within environment $\mathcal{M}$ is:
\begin{equation}
    \text{CR}(\mathcal{M}, \alpha) = \mathbb{P}_{\tau_1, \tau_2 \sim \mathcal{T}} \left[ \cos(g(\tau_1), g(\tau_2)) < 0 \right]
\end{equation}
\end{definition}

\begin{proposition}[Conflict Rate Bounds]
For a multi-objective environment with conflicting objectives:
\begin{enumerate}
    \item If objectives are perfectly aligned: $\text{CR} = 0$
    \item If objectives are orthogonal: $\text{CR} \approx 0.5$
    \item If objectives are anti-aligned: $\text{CR} \approx 1$
\end{enumerate}
\end{proposition}

\subsection{Empirical Gradient Conflict Analysis}

We measure gradient conflict empirically by:
\begin{enumerate}
    \item Sampling $N$ trajectories from environment $\mathcal{M}$ with policy $\pi_\theta$
    \item Computing per-trajectory gradients $\{g(\tau_1), \ldots, g(\tau_N)\}$
    \item Computing pairwise cosine similarities
    \item Counting pairs with negative similarity (conflict)
\end{enumerate}

\begin{table}[h]
\centering
\caption{Empirical gradient conflict rates within each environment (same $\alpha = 0.5$, random policy)}
\label{tab:conflict_rates}
\begin{tabular}{lccc}
\toprule
Environment & Conflict Rate & Mean Cosine & Std Dev \\
\midrule
Deep Sea Treasure & 47.2\% & 0.02 & 0.41 \\
Four Room & 38.5\% & 0.12 & 0.35 \\
Minecart & 35.1\% & 0.18 & 0.28 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings:}
\begin{itemize}
    \item \textbf{DST has highest conflict} (47\%): Nearly half of trajectory pairs produce opposing gradients, due to the discrete Pareto front and sparse rewards.
    \item \textbf{FourRoom has moderate conflict} (38\%): The two goals create natural gradient opposition.
    \item \textbf{Minecart has lowest conflict} (35\%): Continuous dynamics lead to smoother gradient distributions.
\end{itemize}

\subsection{Implications for Domain Discovery}

The intra-domain gradient conflict has important implications:

\subsubsection{1. High Variance in Gradient Estimates}

When trajectories conflict, the mean gradient has high variance:
\begin{equation}
    \text{Var}[\hat{g}] = \frac{1}{N} \text{Var}[g(\tau)]
\end{equation}

This explains why DST is harder to cluster: its gradient estimates are noisy.

\subsubsection{2. Need for Distributional Comparison}

Comparing mean gradients (cosine similarity) loses information about the conflict structure. This motivates using \textbf{Optimal Transport} to compare the full gradient \textit{distributions}:
\begin{equation}
    d(\mathcal{M}_i, \mathcal{M}_j) = W_2(\mu_i, \mu_j)
\end{equation}
where $\mu_i$ is the distribution of gradients from environment $i$.

\subsubsection{3. Conflict as a Domain Signature}

Interestingly, the \textit{pattern} of gradient conflict is itself a domain signature:
\begin{itemize}
    \item DST: High conflict, discrete modes
    \item FourRoom: Moderate conflict, bimodal distribution
    \item Minecart: Low conflict, unimodal distribution
\end{itemize}

Environments with similar conflict patterns cluster together.

\subsection{Visualizing Intra-Domain Conflict}

Figure~\ref{fig:gradient_conflict} illustrates the gradient conflict phenomenon:

\begin{figure}[h]
\centering
% Include figure here
\caption{Intra-domain gradient conflict. (a) Two trajectories in Deep Sea Treasure reaching different treasures produce opposing gradients. (b) Distribution of pairwise gradient cosine similarities within each environment. (c) Gradient vectors in PCA space, colored by trajectory outcome.}
\label{fig:gradient_conflict}
\end{figure}

\subsection{Mathematical Analysis}

\begin{theorem}[Gradient Conflict in Multi-Objective MDPs]
\label{thm:conflict}
Consider a multi-objective MDP with objectives $J_1$ and $J_2$. Let $\theta_{12} = \angle(\nabla J_1, \nabla J_2)$ be the angle between objective gradients. For a scalarized objective with weight $\alpha$:
\begin{equation}
    \nabla J_\alpha = \alpha \nabla J_1 + (1-\alpha) \nabla J_2
\end{equation}

The variance of the scalarized gradient is:
\begin{equation}
    \text{Var}[\nabla J_\alpha] = \alpha^2 \text{Var}[\nabla J_1] + (1-\alpha)^2 \text{Var}[\nabla J_2] + 2\alpha(1-\alpha) \text{Cov}[\nabla J_1, \nabla J_2]
\end{equation}

When objectives conflict ($\theta_{12} > 90°$), the covariance term is negative, but the individual variances dominate, leading to high overall variance.
\end{theorem}

\begin{corollary}[Conflict Maximization]
Gradient conflict is maximized when:
\begin{enumerate}
    \item Objectives are orthogonal or anti-aligned ($\theta_{12} \geq 90°$)
    \item Preference weight is balanced ($\alpha \approx 0.5$)
    \item Reward variance is high (sparse rewards)
\end{enumerate}
\end{corollary}

This explains why DST with $\alpha = 0.5$ exhibits the highest conflict rate.

\subsection{Connection to Gradient-Based Clustering}

The gradient conflict phenomenon directly impacts our clustering approach:

\begin{enumerate}
    \item \textbf{Why mean gradients fail}: High conflict means the mean gradient is a poor summary of the distribution.
    
    \item \textbf{Why OT helps}: Wasserstein distance captures the full distribution, including multi-modal structure from conflicting trajectories.
    
    \item \textbf{Why more samples help}: Averaging over many trajectories reduces variance, but the underlying conflict structure remains.
\end{enumerate}

\paragraph{Practical Recommendation.} For environments with high gradient conflict:
\begin{itemize}
    \item Use more gradient samples (we use 50 per domain)
    \item Average over multiple policy initializations
    \item Use distributional metrics (Sliced Wasserstein) rather than point estimates (cosine similarity)
\end{itemize}
