# Extreme Conflict with Not Aligned Preferences (NAL)
# MDP1: long_cp (p=0.95) prioritizes MAKESPAN (alpha=0.9)
# MDP2: wide (p=0.05) prioritizes ENERGY (alpha=0.9)
# This creates true multi-objective conflict
experiment:
  name: "extreme_conflict_NAL"
  seed: 12345
  output_dir: "logs"
  device: "cpu"

training:
  total_timesteps: 100000
  learning_rate: 0.00025
  num_envs: 4
  num_steps: 128
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 2
  update_epochs: 4
  clip_coef: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  anneal_lr: true

evaluation:
  test_every_iters: 10
  robust_eval_alpha: 0.25

domain:
  longcp_config: "data/rl_configs/extreme_longcp_p095_bottleneck.json"  # MDP1: very sequential
  wide_config: "data/rl_configs/extreme_wide_p005_bottleneck.json"      # MDP2: very parallel
  host_specs_file: "data/host_specs.json"

seed_control:
  mode: "controlled"

variant:
  name: "hetero"

# NO adversarial search - use fixed extreme MDPs
adversarial:
  enabled: false

# Multi-objective preferences - NOT ALIGNED
morl:
  # MDP1 (long_cp): prioritize makespan
  alpha_makespan_mdp1: 0.9
  alpha_energy_mdp1: 0.1
  
  # MDP2 (wide): prioritize energy
  alpha_makespan_mdp2: 0.1
  alpha_energy_mdp2: 0.9

trajectory:
  enabled: false

logging:
  tensorboard: true
  log_every: 5
  grad_log_every: 5  # Track gradient conflict closely
